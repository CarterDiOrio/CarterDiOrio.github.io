"use strict";(self.webpackChunkportfolio=self.webpackChunkportfolio||[]).push([[555],{7128:function(e,t,a){a.r(t),a.d(t,{Head:function(){return u},default:function(){return p}});var n=a(1151),i=a(7294),r=(a(6222),a(795));function o(e){const t=Object.assign({h1:"h1",a:"a",span:"span",h2:"h2",p:"p",h3:"h3",img:"img",ol:"ol",li:"li"},(0,n.a)(),e.components);return i.createElement(i.Fragment,null,i.createElement(t.h1,{id:"analysis-of-fiducial-markers-and-camera-calibration",style:{position:"relative"}},i.createElement(t.a,{href:"#analysis-of-fiducial-markers-and-camera-calibration","aria-label":"analysis of fiducial markers and camera calibration permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Analysis of Fiducial Markers and Camera Calibration"),"\n",i.createElement("br"),"\n",i.createElement(r.iz),"\n",i.createElement("br"),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto"}},i.createElement("video",{autoPlay:!0,loop:!0,muted:!0,playsInline:!0,controls:!0,style:{maxHeight:"100%",maxWidth:"100%"}},i.createElement("source",{src:"/project.mp4",type:"video/mp4"}))),"\n",i.createElement("br"),"\n",i.createElement(r.iz),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"fiducial-analysis",style:{position:"relative"}},i.createElement(t.a,{href:"#fiducial-analysis","aria-label":"fiducial analysis permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fiducial Analysis"),"\n",i.createElement(t.p,null,"Github repository: ",i.createElement(t.a,{href:"https://github.com/CarterDiOrio/FiducialMarkers"},"https://github.com/CarterDiOrio/FiducialMarkers")),"\n",i.createElement(t.p,null,"The original aim of of the project was to find the best method to detect an April tag using a Intel Realsense D435i camera. To evaluate whether one approach was better or not we needed a method\nto evaluate the accuracy and precision of the fiducial and detection method combo."),"\n",i.createElement(t.h3,{id:"experimental-setup",style:{position:"relative"}},i.createElement(t.a,{href:"#experimental-setup","aria-label":"experimental setup permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Experimental Setup"),"\n",i.createElement(t.p,null,"To provide a ground truth transform between the camera and the tag for accuracy evaluation I used a Vicon optical tracking system. Since  our system has a small workspace it can measure position with an accuracy\non the order of tens of microns."),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/vicon.jpg",alt:"Image"})," ")),"\n",i.createElement(t.p,null,"Tracking markers were then mounted to both the camera and the fiducial. This setup still does not directly give the ground truth transform. It gives us the transform between\nthe markers attached to the camera (the hand) and the markers attached to the fiducial (the mount). To get a measurement of T_camera_fiducial from the Vicon we need to calibrate\ntwo more transforms: the camera relative to the hand (T_hand_eye) and the fiducial relative to the mount (T_mount_fiducial). Knowing these two transforms completes the loop and establishes\na ground truth estimate as shown below with the vicon as the world frame."),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/handeyegraph.png",alt:"Image"})," ")),"\n",i.createElement(t.p,null,"One major issue with this is that if we perform the calibration with a chessboard in the mount, then disassemble the mount to put the tag we want to evaluate in it, the calibration\nis now invalid. Instead of paper tags that have to be swapped, an non-emissive paper like e-ink display can be used to display the tags. Using a lower level library to access the display\nwe can exactly control how each fiducial is rendered, this ensures the T_mount_fiducial\ntransform is transferable since we can render each calibration pattern or fiducial such that their origins are all in the same location. An added benefit is it also removes the additional position\nerror that results from trying to repeat the same trajectory with each tag since for each position all the tags can be displayed on the display automatically."),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto",display:"flex",justifyContent:"center",alignItems:"center"}},i.createElement("video",{autoPlay:!0,loop:!0,muted:!0,playsInline:!0,controls:!0,style:{maxHeight:"400px"}},i.createElement("source",{src:"/eink_testing_comp.mp4",type:"video/mp4"}))),"\n",i.createElement(t.p,null,"At its core the optimal calibration must minimize the reprojection error of all features as shown in the equation below, where every fiducial feature X has a corresponding image feature y, and p() is the\nprojection function. Using a dense (26x26) chessboard as the calibration pattern I performed nonlinear optimization and the result was a 0.33 mean pixel reprojection error and a 0.25 millimeter mean translational error\nof the extrinsics."),"\n",i.createElement("div",{style:{maxWidth:"240px",margin:"auto",background:"white","justify-content":"center"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/equation.png",alt:"Image"})," ")),"\n",i.createElement(t.h3,{id:"precision-results",style:{position:"relative"}},i.createElement(t.a,{href:"#precision-results","aria-label":"precision results permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Precision Results"),"\n",i.createElement(t.p,null,"For each of the three fiducials under test, Chessboards, April tags, and ArUco markers, I assessed their accuracy over increasing distance and varied orientations."),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto",background:"white","justify-content":"center"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/chessboard_results.png",alt:"Image"})," ")),"\n",i.createElement(t.p,null,'Starting off with the chessboard above. Each chessboard is denoted by its number of squares and then the size of each square in millimeters. The sizes happen to be these non-whole numbers because they need to align with\nthe nearest whole number of pixels on the eink display. For my experiment setup the dominant factor in the accuracy of the chessboard is the number of squares. Each "band" of square sizes are perfectly seperated\nfrom each other in this plot and within each band the precision increases with square size. This confirms the common knowledge of more and larger corners is better, and that the number of corners should be increased\ntill diminishing or negative returns.'),"\n",i.createElement("div",{style:{maxWidth:"1000px",margin:"auto",background:"white",display:"flex",justifyContent:"center"}},i.createElement("img",{src:"/april_results.png",alt:"April Results",style:{maxWidth:"50%",height:"auto",marginRight:"10px"}}),i.createElement("img",{src:"/aruco_results.png",alt:"Aruco Results",style:{maxWidth:"50%",height:"auto"}})),"\n",i.createElement(t.p,null,"Above on the left and right we have the April tag and ArUco results respectively. The April tag results have a significant outlier and I left it in to show that, at least in my usage, sometimes the April tag\nlibrary produces significant outliers and that needs to be taken into account. However in their best case with both tags at 75 millimeters the April tags have better precision\nwith 0.04, 0.04, 0.14 millimeters of standard deviation on the XYZ axes compared to 0.05, 0.05, and 0.17 millimeters of standard deviation on the ArUco markers. These results are close enough that it could be\nargued it is within margin of error, but the best advice I could give is to benchmark it for your specific scenario. For example, I noticed the April tag detector works better on with the e-ink display (it has slight\nglare) than the ArUco detector does."),"\n",i.createElement(t.h3,{id:"conclusion",style:{position:"relative"}},i.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",i.createElement(t.p,null,"Planar fiducials like chessboards, April tags, and ArUco markers, can be very precise depending on your needs. With larger tags and a limited workspace you can expect sub-millimeter std deviations\non all three axes with deviations getting as low as ~0.05mm, ~0.05mm, ~0.17mm on the XYZ axes respectively. Even if your workspace falls outside\nthis definition, it has been shown that the precision scales linearly over distance within reasonable workspace and tag orientation envelope. However\nyou can expect a large dropoff in precision and detection suddenly at further distances according to the April Tag 2 and ArUco papers. Some of this dropoff is attributed\nto effects like feature detection noise increasing non linearly with the distance to the feature."),"\n",i.createElement(t.p,null,"It is of note that accuracy is extremely difficult to test empirically and it is why there is no \"Accuracy Resuls\" section. Most major fiducial papers, even those claiming high accuracy like Rune Tags,\nuse synthetically generated images for their experimental accuracy validation. These are careful simulations, but a method does not exist to empirically evaluate it on\nyour specific camera setup. This is because you need the ground truth transform between the camera's center of projection and the tag. The tag's world position\ncan either be surveyed or measured in a variety of ways, but the camera's center cannot be directly measured. It needs to be estimated using information from the camera itself,\nand as far as I could find an extrinsic calibration method does not exist that gives sufficient accuracy and precision guarantees to meaningfully measure the accuracy of the tags.\nEmpirically measuring the accuracy of tags is getting further into the area of serious metrology and statistics than the realm of typical computer vision and any further work needs\nto treat it as such."),"\n",i.createElement(t.p,null,"I also did not cover the rotational precision because planar fiducial markers also need to deal with a orientation ambiguity that exists in planar PnP (See [2]).\nThis effects all standard planner patterns including chessboards but to a lesser extent. Solutions to this vary as covered by [2], but include modifications to the tag, averaging, filtering, and tracking. However,\nignoring this effect, it scales linearly like the translational error within a reasonable workspace."),"\n",i.createElement(t.p,null,"I did not have the time or setup required to explore this fully, but for future work, if you could prove that the relationship is extrapolatable back to the normal case it might be possible to use a camera that has significantly\nhigher resolution to perform the extrinsic calibration then set it to a significantly lower resolution and move the tag far enough away that the tags error becomes the dominating factor. The one issue with this\nis that tags are quite robust to lower resolutions. As shown in the chart below, I measured the std deviation of a stationary 75mm April tag at half a meter while continually decreasing the resolution by scailing the image\ndown with the X axis being the percentage of 1080p. The reason I resorted to scailing is because most cameras (like the Realsense I was using) only have a few fixed resolutions they can operate at and it is not enough to get a good line fit. To make sure I was not\nexaggerating the tags effectiveness by introducing interpolated information due to downscailing from a higher resolution I repeated the test but started at 720p and that is shown in orange and it matches the 1080p results. This is a great result\nfor the tags, but it means this thrust of future research would require a camera that could go up to a significantly higher resolution."),"\n",i.createElement("div",{style:{maxWidth:"600px",margin:"auto",background:"white","justify-content":"center"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/april_tag_resolution.png",alt:"Image"})," ")),"\n",i.createElement("br"),"\n",i.createElement(r.iz),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"mrcal",style:{position:"relative"}},i.createElement(t.a,{href:"#mrcal","aria-label":"mrcal permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"MrCal"),"\n",i.createElement(t.p,null,"Github repository: ",i.createElement(t.a,{href:"https://github.com/CarterDiOrio/mrcal_ros"},"https://github.com/CarterDiOrio/mrcal_ros")),"\n",i.createElement(t.p,null,"Although it's challenging to empirically measure, one of the most critical factors affecting the accuracy of your tag's position measurements is your camera's intrinsic parameters. There's no substitute for using a robust framework and dedicating the effort to achieve precise calibration.\nOne of the best open source frameworks that attempts to give statistical feedback and guarantees is MrCal [3] developed by NASA's JPL. MrCal's\ndocuments and tutorial explains all of its analyses in detail, but I will give a short example about how it can approve even a well behaved lens as motivation. The plots in this section are from MrCal, which also serves as the source of all background information. For this example, I used approximately 400 images of a dense chessboard (26x26) as the calibration data"),"\n",i.createElement(t.p,null,"The Intel RealSense D435i's color lens does not come with distortion coefficients (see https://github.com/IntelRealSense/librealsense/issues/8325 and https://github.com/IntelRealSense/librealsense/issues/1430). Intel's\nopinion is that it reduces about 1px of reprojection error at the extremess, and at first glance, the lens appears to be well-behaved. However, when we use a pinhole model and examine the reprojection error residuals from calibration, we observe clustering in the residual magnitudes, as shown below. Clustering of this kind indicates lens distortion, assuming other factors like rolling shutter have been accounted for. Since our chessboard corner detection error is roughly normally distributed, a fully modeled lens should also yield normally distributed residual errors in both magnitude and direction, which should appear as noise"),"\n",i.createElement("div",{style:{maxWidth:"600px",margin:"auto",background:"white","justify-content":"center"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/pinhole_magnitudes.png",alt:"Image"})," ")),"\n",i.createElement(t.p,null,"As for the residual directions, shown below for the pinhole model, they also exhibit a high degree of clustering. It's important to emphasize that this isn't due to a poor calibration but rather to modeling error. No amount of refining the pinhole parameters will resolve this issue because the D435i's color lens simply isn't a pinhole lens. Even if you don't require the enhanced accuracy provided by a more complex model, MrCal offers tools to help you make an informed decision."),"\n",i.createElement("div",{style:{maxWidth:"600px",margin:"auto",background:"white","justify-content":"center"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/pinhole_directions.png",alt:"Image"})," ")),"\n",i.createElement(t.p,null,"Moving onto a richer model, if we rerun the calibration with MrCal's Splined-Stereographic lens model those two plots turn into noise below like we would expect in the ideal case, and this is a result\nof modeling the lens correctly."),"\n",i.createElement("div",{style:{maxWidth:"1000px",margin:"auto",background:"white",display:"flex",justifyContent:"center"}},i.createElement("img",{src:"/spline_magnitudes.png",alt:"April Results",style:{maxWidth:"50%",height:"auto",marginRight:"10px"}}),i.createElement("img",{src:"/spline_directions.png",alt:"Aruco Results",style:{maxWidth:"50%",height:"auto"}})),"\n",i.createElement(t.p,null,"A benefit of correcting the modeling errors is our reprojection error decreases subsantially. The two histogram's below show the distribution of the residual error. On top is the pinhole model with an RMS reprojection error of 0.7 pixels, and on the bottom is\nthe spline model with an RMS reprojectin error of less than 0.1 pixels. A concrete example of the spline model significantly helping was in the extrinsic calibration of the fiducial analysis setup. Using the MrCal model compared to the built in realsense intrinsics\nbrought the translational error of the extrinsics down to 0.25 millimeters from 0.75 millimeters with no additional changes."),"\n",i.createElement("div",{style:{maxWidth:"1000px",margin:"auto",background:"white",justifyContent:"center"}},i.createElement(t.p,null,i.createElement(t.img,{src:"/pinhole_histogram.png",alt:"Image"})," ","\n",i.createElement(t.img,{src:"/spline_histogram.png",alt:"Image"})," ")),"\n",i.createElement("br"),"\n",i.createElement(r.iz),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"targetless-hand-eye-calibration",style:{position:"relative"}},i.createElement(t.a,{href:"#targetless-hand-eye-calibration","aria-label":"targetless hand eye calibration permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Targetless Hand Eye Calibration"),"\n",i.createElement(t.p,null,"Github repository: ",i.createElement(t.a,{href:"https://github.com/CarterDiOrio/SfMHandEye"},"https://github.com/CarterDiOrio/SfMHandEye")),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto",display:"flex",justifyContent:"center",alignItems:"center"}},i.createElement("video",{autoPlay:!0,loop:!0,muted:!0,playsInline:!0,controls:!0,style:{maxHeight:"400px"}},i.createElement("source",{src:"/cloud_cropped.mp4",type:"video/mp4"}))),"\n",i.createElement(t.p,null,"As part of a new research thrust for exploring using a highly accurate robot arm, the Mechademic 500, to improve on and make transferable hand eye calibrations, I implemented a targetless hand eye calibration routine. It works by\nusing Structure From Motion to reconstruct the environment from random camera poses with the robot's motion providing the scale of the scene. Once the environment and camera positions are reconstructed, the environment itself takes the place of the calibration target to\ngive the trajectory of the camera in the camera frame. The hand eye calibration is then found using Bundle Adjustment. The above video is the 3D point cloud from the SfM reconstruction of the MSR lab environment with the green dots being the registered camera\nposes. The below video showcases the hand eye calibration by commanding the robot to pivot about the color camera sensor of the camera."),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto",display:"flex",justifyContent:"center",alignItems:"center"}},i.createElement("video",{autoPlay:!0,loop:!0,muted:!0,playsInline:!0,controls:!0,style:{maxHeight:"400px"}},i.createElement("source",{src:"/robot.mp4",type:"video/mp4"}))),"\n",i.createElement(t.p,null,"To evaluate the hand eye calibration accuracy I used a similar test to [5]. I had the robot move to poses that observed a ChArUco board. The ChArUco board provided the\nmovement in the camera frame and then the average translation errors where found by comparing the expected and actual camera motion using the Hand Eye transform. It\nachieved a mean translational error of 1.7 millimeters and a mean rotational error of 0.37 degrees. To compare this against an industry solution, Zivid an industrial 3D\ncamera manufacturer, provides a target based hand eye calibration solution and this targetless solution exceeds it in rotational accuracy while falling slightly behind\nin translation accuracy (See Zivid Results here: https://blog.zivid.com/importance-of-3d-hand-eye-calibration)"),"\n",i.createElement("div",{style:{maxWidth:"800px",margin:"auto",display:"flex",justifyContent:"center",alignItems:"center"}},i.createElement("video",{autoPlay:!0,loop:!0,muted:!0,playsInline:!0,controls:!0,style:{maxHeight:"400px"}},i.createElement("source",{src:"/robot_arm_testing.mp4",type:"video/mp4"}))),"\n",i.createElement(t.h2,{id:"citations",style:{position:"relative"}},i.createElement(t.a,{href:"#citations","aria-label":"citations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Citations"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Enebuse I, Ibrahim BKSMK, Foo M, Matharu RS, Ahmed H. Accuracy evaluation of hand-eye calibration techniques for vision-guided robots. PLoS One. 2022;17(10):e0273261. Published 2022 Oct 19. doi:10.1371/journal.pone.027326"),"\n",i.createElement(t.li,null,'H. Tanaka, Y. Sumi and Y. Matsumoto, "A solution to pose ambiguity of visual markers using MoirÃ© patterns," 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, Chicago, IL, USA, 2014, pp. 3129-3134, doi: 10.1109/IROS.2014.6942995.'),"\n",i.createElement(t.li,null,"https://mrcal.secretsauce.net/"),"\n",i.createElement(t.li,null,'M. Krogius, A. Haggenmiller and E. Olson, "Flexible Layouts for Fiducial Tags," 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 1898-1903, doi: 10.1109/IROS40897.2019.8967787.'),"\n",i.createElement(t.li,null,'H. Xie, C. -t. Pang, W. -l. Li, Y. -h. Li and Z. -p. Yin, "Hand-eye calibration and its accuracy analysis in robotic grinding," 2015 IEEE International Conference on Automation Science and Engineering (CASE), Gothenburg, Sweden, 2015, pp. 862-867, doi: 10.1109/CoASE.2015.7294189.'),"\n"))}var s=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.a)(),e.components);return t?i.createElement(t,e,i.createElement(o,e)):o(e)},l=a(4842),c=a(1885),h=a(7827),d=a(5277);const m=e=>{let{data:t,children:a}=e;return(0,d.BX)(l.Z,{children:[!t.mdx.frontmatter.underconstruction&&(0,d.BX)("div",{className:c.N,children:[(0,d.tZ)("div",{sx:{paddingRight:"16px"},children:(0,d.BX)(r.Zb,{className:c.J,variant:"tableOfContents",children:[t.mdx.frontmatter.github&&(0,d.BX)("p",{children:["Github: ",(0,d.tZ)("a",{href:t.mdx.frontmatter.github,sx:{color:"primary"},children:"Repository"})," "]}),(0,d.tZ)(h.Z,{items:t.mdx.tableOfContents.items})]})}),(0,d.tZ)("div",{children:(0,d.tZ)(n.Z,{children:a})})]}),t.mdx.frontmatter.underconstruction&&(0,d.tZ)("div",{children:(0,d.tZ)("h2",{children:"Under Construction. Please check back later for updates"})})]})},u=e=>{let{data:t}=e;return(0,d.tZ)("title",{children:t.mdx.frontmatter.title})};function p(e){return i.createElement(m,e,i.createElement(s,e))}},7827:function(e,t,a){var n=a(5785),i=a(7294),r=a(795),o=a(5893);const s=e=>e.reduce(((e,t)=>(t.url&&e.push(t.url.slice(1)),t.items&&e.push.apply(e,(0,n.Z)(s(t.items))),e)),[]),l=e=>{let{items:t,activeId:a,depth:n}=e;console.log(t);return(0,o.jsx)("ul",{style:{listStyleType:"none",paddingLeft:"16px"},children:t.map((e=>{const t=e.url==="#"+a?"tableOfContentsActive":"tableOfContents";return(0,o.jsxs)("li",{children:[(0,o.jsx)(r.OL,{href:e.url,variant:""+t,style:{fontSize:18-3*n+"px",opacity:""+(1-.2*n)},children:e.title}),e.items&&(0,o.jsx)(l,{items:e.items,activeId:a,depth:n+1})]},e.url)}))})};t.Z=e=>{let{items:t}=e;const a=(e=>{let{ids:t}=e;const[a,n]=i.useState("");return i.useEffect((()=>{const e=t.map((e=>document.getElementById(e))),a=new IntersectionObserver((e=>{e.forEach((e=>{if(e.isIntersecting){const t=e.target.getAttribute("id");n(t)}}))}),{rootMargin:"0% 0% -80% 0%"});return e.forEach((e=>a.observe(e))),()=>a.disconnect()}),[]),a})({ids:s(t)});return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("h2",{children:"Contents"}),(0,o.jsx)(l,{items:t[0].items,activeId:a,depth:0})]})}},1885:function(e,t,a){a.d(t,{J:function(){return i},N:function(){return n}});var n="project-module--projectLayout--0a8d9",i="project-module--tableOfContents--da931"}}]);
//# sourceMappingURL=component---src-templates-project-tsx-content-file-path-src-projects-camera-calibration-mdx-eeccbffe40df981b9f47.js.map