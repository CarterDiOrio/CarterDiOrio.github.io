---
title: "Camera Calibration"
subtitle: "An Exploration of Fiducials and Image Geometry"
date: "2023-08-25"
url: "camera-calibration"
picture: "/project.mp4"
tags: "Fiducials, PnP, Structure from Motion, Calibration, Intrinsics, Extrinsics, Bundle Adjustment"   
order: "0"
featured: true
underconstruction: false
---

import { YouTube } from 'mdx-embed'
import { Divider } from 'theme-ui'

# Camera Calibration
<br/>
<Divider/>
<br/>

<div style={{
    maxWidth: "800px",
    margin: "auto"
}}>
    <video
        autoPlay
        loop
        muted
        playsInline
        controls
        style={{
            maxHeight: "100%",
            maxWidth: "100%",
        }}
    >
        <source
            src="/project.mp4"  
            type="video/mp4"
        />
    </video>
</div>


<br/>
<Divider/>
<br/>

## Fiducial Analysis


The original aim of of the project was to find the best method to detect an April tag using a Intel Realsense D435i camera. To evaluate whether one approach was better or not we needed a method 
to evaluate the accuracy and precision of the fiducial and detection method combo. 

### Experimental Setup

To provide a ground truth transform between the camera and the tag for accuracy evaluation I used a Vicon optical tracking system. Since  our system has a small workspace it can measure position with an accuracy
on the order of tens of microns.

<div style={{
    maxWidth: "800px",
    margin: "auto"
}}>
    ![Image](/vicon.jpg) {}
</div>

Tracking markers were then mounted to both the camera and the fiducial. This setup still does not directly give the ground truth transform. It gives us the transform between
the markers attached to the camera (the hand) and the markers attached to the fiducial (the mount). To get a measurement of T_camera_fiducial from the Vicon we need to calibrate
two more transforms: the camera relative to the hand (T_hand_eye) and the fiducial relative to the mount (T_mount_fiducial). Knowing these two transforms completes the loop and establishes 
a ground truth estimate as shown below with the vicon as the world frame.

<div style={{
    maxWidth: "800px",
    margin: "auto"
}}>
    ![Image](/handeyegraph.png) {}
</div>

One major issue with this is that if we perform the calibration with a chessboard in the mount, then disassemble the mount to put the tag we want to evaluate in it, the calibration
is now invalid. Instead of paper tags that have to be swapped, an non-emissive paper like e-ink display can be used to display the tags. Using a lower level library to access the display
we can exactly control how each fiducial is rendered, this ensures the T_mount_fiducial
transform is transferrable since we can render each calibration pattern or fiducial such that their origins are all in the same location. An added benefit is it also removes the additional position 
error that results from trying to repeat the same trajectory with each tag since for each position all the tags can be displayed on the display automatically.

<div style={{
    maxWidth: "800px",
    margin: "auto",
    display: "flex",
    justifyContent: "center",
    alignItems: "center"
}}>
    <video
        autoPlay
        loop
        muted
        playsInline
        controls
        style={{
            maxHeight: "400px",
        }}
    >
        <source
            src="/eink_testing_comp.mp4"  
            type="video/mp4"
        />
    </video>
</div>

At its core the optimal calibration must minimize the reprojection error of all features as shown in the equation below, where every fiducial feature X has a corresponding image feature y, and p() is the 
projection function. Using a dense (26x26) chessboard as the calibration pattern I performed nonlinear optimization and the result was a 0.33 mean pixel reprojection error and a 0.25 millimeter mean translational error
of the extrinsics.

<div style={{
    maxWidth: "240px",
    margin: "auto",
    background: "white",
    "justify-content": "center"
}}>
    ![Image](/equation.png) {}
</div>

### Precision Results

For each of the three fiducials under test, Chessboards, April tags, and ArUco markers, I assessed their accuracy over increasing distance and varied orientations.

<div style={{
    maxWidth: "800px",
    margin: "auto",
    background: "white",
    "justify-content": "center"
}}>
    ![Image](/chessboard_results.png) {}
</div>

Starting off with the chessboard above. Each chessboard is denoted by its number of squares and then the size of each square in millimeters. The sizes happen to be these non-whole numbers because they need to align with 
the nearest whole number of pixels on the eink display. For my experiment setup the dominant factor in the accuracy of the chessboard is the number of squares. Each "band" square size lines are perfectly seperated
from eachother in this plot and within each band the precision increases with square size. This confirms the common knowledge of more and larger corners is better, and that the number of corners should be increased
till diminishing or negative returns. 

<div style={{
    maxWidth: "1000px",
    margin: "auto",
    background: "white",
    display: "flex",
    justifyContent: "center"
}}>
    <img src="/april_results.png" alt="April Results" style={{ maxWidth: "50%", height: "auto", marginRight: "10px" }} />
    <img src="/aruco_results.png" alt="Aruco Results" style={{ maxWidth: "50%", height: "auto" }} />
</div>

Above on the left and right we have the April tag and ArUco results respectively. The April tag results have a significant outlier and I left it in to show that, at least in my usage, sometimes the April tag 
library produces significant outliers and that needs to be taken into account. However in their best case with both tags at 75 millimeters the April tags have better precision 
with 0.04, 0.04, 0.14 millimeters of standard deviation on the XYZ axes compared to 0.05, 0.05, and 0.17 millimeters of standard deviation on the ArUco markers. These results are close enough that it could be
argued it is within margin of error, but the best advice I could give is to benchmark it for your specific scenario. For example, I noticed the April tag detector works better on with the e-ink display (it has slight
glare) than the ArUco detector does.

### Conclusion

Planar fiducials like chessboards, April tags, and ArUco markers, can be very precise depending on your needs. 
It is expanded on below, but at the lowest end with larger tags and a limited workspace you can expect sub-millimeter std deviations
on all three axes with deviations getting as low as ~0.05mm, ~0.05mm, ~0.17mm on the XYZ axes respectively. Even if your workspace falls outside
this definition, it has been shown that the precision scales linearly over distance within reasonable workspace and tag orientation envelope. However
you can expect a large dropoff in precision and detection suddenly at further distances according to the April Tag 2 and ArUco papers. Some of this dropoff is attributed
to effects like feature detection noise increasing non linearly with distance to the feature.

It is of note that accuracy is extremely difficult to test empirically and it is why there is no "Accuracy Resuls" section. Most major fiducial papers, even those claiming high accuracy like Rune Tags, 
use synthetically generated images for their experimental accuracy validation. These are careful simulations, but a method does not exist to empirically evaluate it on
your specific camera setup. This is because you need the ground truth transform between the camera's center of projection and the tag. The tag's world position
can either be surveyed or measured in a variety of ways, but the camera's center cannot be directly measured. It needs to be estimated using information from the camera itself,
and as far as I could find an extrinsic calibration method does not exist that gives sufficient accuracy and precision guarantees to meaningfully measure the accuracy of the tags. 
Emperically measuring the accuracy of tags is getting further into the area of serious metrology and statistics than the realm of typical computer vision and any further work needs 
to treat it as such. 

I also did not cover the rotational precision because planar fiducial markers also need to deal with a orientation ambiguity that exists in planar PnP (See [2]).
This effects all standard planner patterns including chessboards but to a lesser extent. Solutions to this vary as covered by [2], but include modifications to the tag, averaging, filtering, and tracking. However,
ignoring this effect, it scales linearly like the translational error within a reasonable workspace.


I did not have the time or setup required to explore this fully, but for future work, if you could prove that the relationship is extrapolatable back to the normal case it might be possible to use a camera that has significantly
higher resolution to perform the extrinsic calibration then set it to a significantly lower resolution and move the tag far enough away that the tags error becomes the dominating factor. The one issue with this
is that tags are quite robust to lower resolutions. As shown in the chart below, I measured the std deviation of a stationary 75mm April tag at half a meter while continually decreasing the resolution by scailing the image 
down with the X axis being the percentage of 1080p. The reason I resorted to scailing is because most cameras (like the Realsense I was using) only have a few fixed resolutions they can operate at and it is not enough to get a good line fit. To make sure I was not
exaggerating the tags effectiveness by essential downscailing from a higher resolution I repeated the test but started at 720p and that is shown in orange and it matches the 1080p results. This is a great result
for the tags, but it means this thrust of future research would require a camera that could go up to a significantly higher resolution.

<div style={{
    maxWidth: "600px",
    margin: "auto",
    background: "white",
    "justify-content": "center"
}}>
    ![Image](/april_tag_resolution.png) {}
</div>

## MrCal

While its not easy to empirically measure, a extremely important and major contributor to how accurate your measure of a tag's position is your camera's intrinsics. There is no substitute for using a good
framework and putting in the effort to get a good calibration. One of the best, if not the best, open source frameworks that attempts to give statistical feedback and guarantees is MrCal [3] developed by NASA's JPL. MrCal
documents and explains all of its analysis in detail and nothing will be better than just reading its documentation and tutorials, but I will give a short example about how it can approve even a well behaved lens. All 
plots in this section are from MrCal, MrCal is also the source of all background information for this section, and I took around 400 images of a dense chessboard that I will be using as the calibration data for the example.

The Intel Realsense D435i's color lens does not ship with distortion coeffecients (see https://github.com/IntelRealSense/librealsense/issues/8325 and https://github.com/IntelRealSense/librealsense/issues/1430). Intel's
opinion is that it reduces about 1px of reprojection error at the extremes and it does appear at a glance to be a well behaved lens. However if we use a pinhole model and look at our reprojection error residuals
from the calibration we get clustering in residual magnitudes as shown below. Clustering of any kind like this is an indicator of lens distortion. Our chessboard corner detection error is roughly normally distributed, 
therefore if we can fully model the lens, the mangitude and direction of our residual errors should also be normally distributed and appear to be noise.

<div style={{
    maxWidth: "600px",
    margin: "auto",
    background: "white",
    "justify-content": "center"
}}>
    ![Image](/pinhole_magnitudes.png) {}
</div>

Speaking of the residual directions, they are shown below for the pinhole model and also show a high degree of clustering. What is important to make note of here, this is not the result of a bad calibration, this is the 
result of modeling error. No amount of perfecting the pinhole parameters will reduce this issue, the D435i's color lens is just not a pinhole lens. Even if you do not need a higher level of accuracy granted by
a richer model, MrCal provides tools to make the decision in an informed manner. 

<div style={{
    maxWidth: "600px",
    margin: "auto",
    background: "white",
    "justify-content": "center"
}}>
    ![Image](/pinhole_directions.png) {}
</div>

Moving onto a richer model, if we rerun the calibration with MrCal's Splined-Stereographic lens model those two plots turn into noise below like we would expect in the ideal case.

<div style={{
    maxWidth: "1000px",
    margin: "auto",
    background: "white",
    display: "flex",
    justifyContent: "center"
}}>
    <img src="/spline_magnitudes.png" alt="April Results" style={{ maxWidth: "50%", height: "auto", marginRight: "10px" }} />
    <img src="/spline_directions.png" alt="Aruco Results" style={{ maxWidth: "50%", height: "auto" }} />
</div>

## Citations

1. Enebuse I, Ibrahim BKSMK, Foo M, Matharu RS, Ahmed H. Accuracy evaluation of hand-eye calibration techniques for vision-guided robots. PLoS One. 2022;17(10):e0273261. Published 2022 Oct 19. doi:10.1371/journal.pone.027326
2. H. Tanaka, Y. Sumi and Y. Matsumoto, "A solution to pose ambiguity of visual markers using Moir√© patterns," 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, Chicago, IL, USA, 2014, pp. 3129-3134, doi: 10.1109/IROS.2014.6942995.
3. https://mrcal.secretsauce.net/
